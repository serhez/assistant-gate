#!/bin/bash
# =============================================================================
# STaR-GATE Evaluation Configuration
# =============================================================================
# Edit these variables before running the evaluation scripts.

# -----------------------------------------------------------------------------
# Project Paths (PROJECT_ROOT is set by scripts before sourcing this file)
# -----------------------------------------------------------------------------
# Virtual environment (uv venv in project root)
export VENV_PATH="${PROJECT_ROOT}/.venv"

# Add src/ to PYTHONPATH so experiments can import paths.py
export PYTHONPATH="${PROJECT_ROOT}/src:${PYTHONPATH:-}"

# -----------------------------------------------------------------------------
# REQUIRED: Your Custom Model on HuggingFace
# -----------------------------------------------------------------------------
# HuggingFace model ID (e.g., "your-username/your-model-name")
# Or local path to model directory
export CUSTOM_MODEL_ID="your-username/your-model-name"

# Short name for this model (used in output directories)
export CUSTOM_MODEL_NAME="custom"

# -----------------------------------------------------------------------------
# REQUIRED: API Keys
# -----------------------------------------------------------------------------
export OPENROUTER_API_KEY="${OPENROUTER_API_KEY:-your-openrouter-api-key}"
export HF_TOKEN="${HF_TOKEN:-your-huggingface-token}"

# -----------------------------------------------------------------------------
# Data Paths
# -----------------------------------------------------------------------------
# Base directory containing the original STaR-GATE data (downloaded from Google Drive)
# This includes prompts, personas, and gold responses used by the paper
export DATA_ROOT="${PROJECT_ROOT}/star-gate-hgx"

# Individual paths (all derived from DATA_ROOT)
export PROMPT_PATH="${DATA_ROOT}/prompts"
export PERSONAS_PATH="${DATA_ROOT}/personas"
export GOLD_PATH="${DATA_ROOT}/gold-responses"
export SIMULATION_PATH="${DATA_ROOT}/simulated-conversations"
export LOGPROBS_PATH="${DATA_ROOT}/log-probs"
export WINRATE_PATH="${DATA_ROOT}/win-rates"

# Model cache: We use HuggingFace's standard cache mechanism.
# Models are automatically cached in $HF_HOME/hub (default: ~/.cache/huggingface/hub)
# To use a custom location, set HF_HOME or HF_HUB_CACHE environment variables.
# On SLURM clusters, this is often pre-configured to a shared filesystem.

# Version tag - must match the folder structure in DATA_ROOT
# The downloaded data uses "star-gate" (not "star-2-bsft")
export VERSION="star-gate"

# -----------------------------------------------------------------------------
# GPU Configuration (for SLURM jobs)
# -----------------------------------------------------------------------------
# Number of GPUs per node
export NUM_GPUS=4

# Tensor parallel size (should divide NUM_GPUS evenly)
export TENSOR_PARALLEL_SIZE=4

# -----------------------------------------------------------------------------
# Evaluation Settings
# -----------------------------------------------------------------------------
# Number of conversation turns
export MAX_TURNS=3

# Top-k conversations to keep
export TOP_K=1

# Batch size for vLLM inference (reduce if OOM)
export VLLM_BATCH_SIZE=100

# Number of return sequences for QA generation
export NUM_RETURN_SEQUENCES=10

# -----------------------------------------------------------------------------
# Human Role-Player Model (for conversation simulation)
# -----------------------------------------------------------------------------
# Backend: "vllm" (local GPU) or "openrouter" (API)
export HUMAN_MODEL_BACKEND="vllm"

# Model ID (format depends on backend):
#   - vLLM: HuggingFace model ID (e.g., "mistralai/Mistral-7B-Instruct-v0.2")
#   - OpenRouter: provider/model (e.g., "deepseek/deepseek-v3.2", "anthropic/claude-3.5-sonnet")
export HUMAN_MODEL_ID="mistralai/Mistral-7B-Instruct-v0.2"

# Short name for logging (optional, derived from model ID if not set)
export HUMAN_MODEL_NAME="mistral-human"

# -----------------------------------------------------------------------------
# Oracle Model (DeepSeek via OpenRouter)
# -----------------------------------------------------------------------------
export ORACLE_MODEL="deepseek/deepseek-v3.2"
export ORACLE_TEMPERATURE=0
export ORACLE_TOP_P=0.95

# -----------------------------------------------------------------------------
# Rating Model (DeepSeek via OpenRouter)
# -----------------------------------------------------------------------------
export RATING_MODEL="deepseek/deepseek-v3.2"
export RATING_TEMPERATURE=0
export RATING_TOP_P=0.95

# -----------------------------------------------------------------------------
# Response Ratings (Step 9)
# -----------------------------------------------------------------------------
# Step 9 evaluates a single model at a time using absolute scoring (1-10).
# The model to evaluate is determined by CUSTOM_MODEL_NAME above.
#
# To evaluate multiple models, run step 09 separately for each:
#   CUSTOM_MODEL_NAME=baseline ./09-get-ratings.sh
#   CUSTOM_MODEL_NAME=my-model ./09-get-ratings.sh
#
# Then compare the test_summary.json files manually.
