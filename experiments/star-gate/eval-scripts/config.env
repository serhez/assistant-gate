#!/bin/bash
# =============================================================================
# STaR-GATE Evaluation Configuration
# =============================================================================
# Edit these variables before running the evaluation scripts.

# -----------------------------------------------------------------------------
# Project Paths (PROJECT_ROOT is set by scripts before sourcing this file)
# -----------------------------------------------------------------------------
# Virtual environment (uv venv in project root)
export VENV_PATH="${PROJECT_ROOT}/.venv"

# Add src/ to PYTHONPATH so experiments can import paths.py
export PYTHONPATH="${PROJECT_ROOT}/src:${PYTHONPATH:-}"

# -----------------------------------------------------------------------------
# REQUIRED: Your Custom Model on HuggingFace
# -----------------------------------------------------------------------------
# HuggingFace model ID (e.g., "your-username/your-model-name")
# Or local path to model directory
export CUSTOM_MODEL_ID="your-username/your-model-name"

# Short name for this model (used in output directories)
export CUSTOM_MODEL_NAME="custom"

# -----------------------------------------------------------------------------
# REQUIRED: API Keys
# -----------------------------------------------------------------------------
export OPENROUTER_API_KEY="${OPENROUTER_API_KEY:-your-openrouter-api-key}"
export HF_TOKEN="${HF_TOKEN:-your-huggingface-token}"

# -----------------------------------------------------------------------------
# Data Paths (all within PROJECT_ROOT by default)
# -----------------------------------------------------------------------------
# Base directory for all data outputs (defaults to PROJECT_ROOT/data)
export DATA_ROOT="${PROJECT_ROOT}/data"

# Individual paths (all derived from DATA_ROOT, which is inside PROJECT_ROOT)
export PROMPT_PATH="${DATA_ROOT}/prompts"
export PERSONAS_PATH="${DATA_ROOT}/personas"
export GOLD_PATH="${DATA_ROOT}/gold-responses"
export SIMULATION_PATH="${DATA_ROOT}/simulated-conversations"
export LOGPROBS_PATH="${DATA_ROOT}/log-probs"
export WINRATE_PATH="${DATA_ROOT}/win-rates"
export MODEL_CACHE_DIR="${DATA_ROOT}/pretrained_models"

# Version tag for this evaluation run
export VERSION="star-2-bsft"

# -----------------------------------------------------------------------------
# GPU Configuration (for SLURM jobs)
# -----------------------------------------------------------------------------
# Number of GPUs per node
export NUM_GPUS=4

# Tensor parallel size (should divide NUM_GPUS evenly)
export TENSOR_PARALLEL_SIZE=4

# -----------------------------------------------------------------------------
# Evaluation Settings
# -----------------------------------------------------------------------------
# Number of conversation turns
export MAX_TURNS=3

# Top-k conversations to keep
export TOP_K=1

# Batch size for vLLM inference (reduce if OOM)
export VLLM_BATCH_SIZE=100

# Number of return sequences for QA generation
export NUM_RETURN_SEQUENCES=10

# -----------------------------------------------------------------------------
# Human Role-Player Model (for conversation simulation)
# -----------------------------------------------------------------------------
# Backend: "vllm" (local GPU) or "openrouter" (API)
export HUMAN_MODEL_BACKEND="vllm"

# Model ID (format depends on backend):
#   - vLLM: HuggingFace model ID (e.g., "mistralai/Mistral-7B-Instruct-v0.2")
#   - OpenRouter: provider/model (e.g., "deepseek/deepseek-v3.2", "anthropic/claude-3.5-sonnet")
export HUMAN_MODEL_ID="mistralai/Mistral-7B-Instruct-v0.2"

# Short name for logging (optional, derived from model ID if not set)
export HUMAN_MODEL_NAME="mistral-human"

# -----------------------------------------------------------------------------
# Oracle Model (DeepSeek via OpenRouter)
# -----------------------------------------------------------------------------
export ORACLE_MODEL="deepseek/deepseek-v3.2"
export ORACLE_TEMPERATURE=0
export ORACLE_TOP_P=0.95

# -----------------------------------------------------------------------------
# Rating Model (DeepSeek via OpenRouter)
# -----------------------------------------------------------------------------
export RATING_MODEL="deepseek/deepseek-v3.2"
export RATING_TEMPERATURE=0
export RATING_TOP_P=0.95

# -----------------------------------------------------------------------------
# Response Ratings (Step 9)
# -----------------------------------------------------------------------------
# Step 9 evaluates a single model at a time using absolute scoring (1-10).
# The model to evaluate is determined by CUSTOM_MODEL_NAME above.
#
# To evaluate multiple models, run step 09 separately for each:
#   CUSTOM_MODEL_NAME=baseline ./09-get-ratings.sh
#   CUSTOM_MODEL_NAME=my-model ./09-get-ratings.sh
#
# Then compare the test_summary.json files manually.
